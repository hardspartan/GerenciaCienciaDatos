#  Dise帽o del Pipeline de Datos

## **Arquitectura del Pipeline de Datos**

### **1. Ingesta y Transformaci贸n de Datos**
En esta fase, se recopilaron y procesaron los datos necesarios para el proyecto. El pipeline de ingesta y transformaci贸n incluy贸 los siguientes pasos:

- **Ingesta de datos**:
  - **Datos principales**: Se carg贸 el dataset principal (`Bd_Transacciones.csv`) desde **Amazon S3**, que conten铆a informaci贸n sobre transacciones ganaderas, incluyendo variables como `Precio_Kilo`, `Peso`, `Raza`, `Tipo_Remate`, y `Fecha`.
  - **Datos externos**: Se integraron datos adicionales desde `dolar.csv` (tipo de cambio del d贸lar) y `clima.csv` (datos meteorol贸gicos como temperatura y precipitaci贸n).

- **Transformaci贸n de datos**:
  - **Limpieza de datos**: Se imputaron valores nulos en columnas como `Raza` y `Marca` con "Desconocido" y "Sin Marca", respectivamente. Adem谩s, se eliminaron outliers en `Precio_Kilo` utilizando el rango intercuart铆lico (IQR).
  - **Integraci贸n de datos**: Se cruzaron los datos principales con `dolar.csv` y `clima.csv` utilizando la columna `Fecha`.
  - **Agrupaci贸n de categor铆as**: Se agruparon categor铆as poco frecuentes en `Raza`, `Marca` y `Categoria` bajo la etiqueta "Otras".
  - **Imputaci贸n de valores nulos en datos externos**: Para las columnas de clima (`Temperatura_Promedio`, `Temperatura_Minima`, `Temperatura_Maxima`, `Precipitacion`), se aplicaron t茅cnicas de interpolaci贸n lineal y relleno hacia adelante y atr谩s (`ffill` y `bfill`).

**Herramientas utilizadas**:
- **AWS Glue**: Para la extracci贸n y transformaci贸n de datos.
- **Pandas**: Para la limpieza y transformaci贸n de datos en el entorno de desarrollo local.

---

### **2. Almacenamiento y Disponibilidad**
Una vez transformados, los datos se almacenaron en un formato accesible para el modelado y an谩lisis. El almacenamiento y disponibilidad se gestionaron de la siguiente manera:

- **Almacenamiento en S3**: Los datos limpios y transformados se almacenaron en **Amazon S3**, en un bucket espec铆fico para el proyecto (`sagemaker-fossa`). Esto permiti贸 un acceso r谩pido y seguro a los datos desde **AWS SageMaker**.
- **Disponibilidad para el modelado**: Los datos se cargaron directamente en **SageMaker** para el entrenamiento y evaluaci贸n del modelo. Se utiliz贸 el formato **CSV** para garantizar la compatibilidad con las herramientas de machine learning.

**Herramientas utilizadas**:
- **Amazon S3**: Para el almacenamiento seguro y escalable de los datos.
- **AWS SageMaker**: Para el acceso y procesamiento de los datos durante el modelado.

---

### **3. Plan de Monitoreo del Pipeline**
Para garantizar la calidad y consistencia de los datos a lo largo del tiempo, se implement贸 un plan de monitoreo del pipeline. Este plan incluy贸 las siguientes acciones:

- **Monitoreo de la ingesta de datos**:
  - Verificaci贸n de la integridad de los datos: Se implementaron checks para asegurar que los datos se cargaran correctamente desde las fuentes externas (`dolar.csv` y `clima.csv`) y que no hubiera p茅rdida de informaci贸n.
  - Detecci贸n de valores nulos: Se monitorearon las columnas clave (`Raza`, `Marca`, `Peso`) para identificar y corregir valores nulos en tiempo real.

- **Monitoreo de la transformaci贸n de datos**:
  - Validaci贸n de transformaciones: Se verific贸 que las transformaciones aplicadas (imputaci贸n de valores nulos, eliminaci贸n de outliers, agrupaci贸n de categor铆as) se ejecutaran correctamente.
  - Auditor铆a de datos: Se realizaron revisiones peri贸dicas para asegurar que los datos transformados cumplieran con los est谩ndares de calidad requeridos.

- **Monitoreo del almacenamiento y disponibilidad**:
  - Disponibilidad de datos: Se implementaron alertas para detectar fallos en la carga de datos en **Amazon S3**.
  - Acceso a los datos: Se monitore贸 el acceso a los datos desde **SageMaker** para asegurar que estuvieran disponibles durante el entrenamiento y evaluaci贸n del modelo.

**Herramientas utilizadas**:
- **AWS CloudWatch**: Para el monitoreo y alertas en tiempo real del pipeline de datos.
- **AWS Glue DataBrew**: Para la validaci贸n y auditor铆a de la calidad de los datos.

---

### **4. Resultados del Pipeline**
El pipeline de datos permiti贸 obtener un dataset limpio y enriquecido, listo para el modelado. Los principales resultados incluyen:

- **Datos limpios**: Se eliminaron valores nulos y outliers, y se imputaron datos faltantes en variables clave como `Raza`, `Marca`, `Peso`, y `Precio_Kilo`.
- **Datos enriquecidos**: Se integraron datos externos como el tipo de cambio del d贸lar y variables clim谩ticas, lo que mejor贸 la capacidad predictiva del modelo.
- **Dataset final**: El dataset final incluy贸 variables como `Fecha`, `Raza`, `Marca`, `Peso`, `Precio_Kilo`, `Dolar`, `Temperatura_Minima`, y `Precipitacion`, listas para ser utilizadas en el entrenamiento del modelo.

---

### **5. Lecciones Aprendidas**
- **Importancia de la calidad de los datos**: La limpieza y transformaci贸n de datos fueron fundamentales para mejorar el rendimiento del modelo.
- **Integraci贸n de datos externos**: La incorporaci贸n de datos como el tipo de cambio del d贸lar y las variables clim谩ticas demostr贸 ser clave para enriquecer el dataset y mejorar las predicciones.
- **Monitoreo continuo**: La implementaci贸n de un plan de monitoreo permiti贸 garantizar la calidad y consistencia de los datos a lo largo del tiempo.

---

Este dise帽o de pipeline de datos fue esencial para alcanzar los objetivos del proyecto, logrando un **RMSE** de **159.97** y un **R虏** de **0.84** en el modelo final.

